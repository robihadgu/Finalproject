<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <title>Deepfake Technology – Social Impact & Ethical Analysis</title>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta
        name="description"
        content="Deepfake technology: technical overview, real-world applications, social impact, and ethical analysis through utilitarian and deontological lenses."
    />
    <style>
        :root {
            --bg: #050814;
            --bg-alt: #0d101f;
            --card: #111523;
            --accent: #4f46e5;
            --accent-soft: rgba(79, 70, 229, 0.15);
            --text: #f9fafb;
            --muted: #9ca3af;
            --border: #1f2937;
            --max-width: 1080px;
        }

        * {
            box-sizing: border-box;
        }

        body {
            margin: 0;
            font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI",
                sans-serif;
            background: radial-gradient(circle at top, #111827 0, #020617 55%);
            color: var(--text);
            line-height: 1.6;
        }

        a {
            color: var(--accent);
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        /* Layout */

        header {
            position: sticky;
            top: 0;
            z-index: 20;
            backdrop-filter: blur(14px);
            background: linear-gradient(
                to bottom,
                rgba(2, 6, 23, 0.92),
                rgba(2, 6, 23, 0.82)
            );
            border-bottom: 1px solid var(--border);
        }

        .nav {
            max-width: var(--max-width);
            margin: 0 auto;
            padding: 0.75rem 1.5rem;
            display: flex;
            align-items: center;
            justify-content: space-between;
            gap: 1rem;
        }

        .nav-title {
            font-weight: 600;
            letter-spacing: 0.04em;
            font-size: 0.9rem;
            text-transform: uppercase;
            color: var(--muted);
        }

        .nav-links {
            display: flex;
            flex-wrap: wrap;
            gap: 0.75rem;
            font-size: 0.85rem;
        }

        .nav-links a {
            padding: 0.25rem 0.6rem;
            border-radius: 999px;
            background: transparent;
            border: 1px solid transparent;
            color: var(--muted);
        }

        .nav-links a:hover {
            border-color: var(--accent-soft);
            background: rgba(15, 23, 42, 0.9);
            color: var(--text);
            text-decoration: none;
        }

        main {
            max-width: var(--max-width);
            margin: 0 auto;
            padding: 2.5rem 1.5rem 4rem;
        }

        section {
            margin-bottom: 3rem;
        }

        h1,
        h2,
        h3 {
            font-weight: 700;
            line-height: 1.2;
        }

        h1 {
            font-size: clamp(2.4rem, 3.2vw + 1.6rem, 3.3rem);
            margin: 0 0 0.75rem;
        }

        h2 {
            font-size: 1.7rem;
            margin-bottom: 0.75rem;
        }

        h3 {
            font-size: 1.15rem;
            margin-bottom: 0.35rem;
        }

        p {
            margin: 0.35rem 0 0.5rem;
            color: var(--muted);
        }

        .hero {
            display: grid;
            grid-template-columns: minmax(0, 3fr) minmax(0, 2.4fr);
            gap: 2rem;
            align-items: center;
            margin-bottom: 3rem;
        }

        .hero-tag {
            display: inline-flex;
            align-items: center;
            gap: 0.4rem;
            padding: 0.25rem 0.6rem;
            border-radius: 999px;
            background: rgba(15, 23, 42, 0.8);
            border: 1px solid var(--border);
            font-size: 0.8rem;
            color: var(--muted);
            margin-bottom: 0.85rem;
        }

        .hero-tag span.dot {
            width: 7px;
            height: 7px;
            border-radius: 999px;
            background: radial-gradient(circle, #22c55e, #15803d);
        }

        .hero-subtitle {
            font-size: 1.02rem;
            max-width: 36rem;
        }

        .meta {
            margin-top: 1rem;
            font-size: 0.85rem;
            color: var(--muted);
        }

        .meta strong {
            color: var(--text);
        }

        .hero-card {
            border-radius: 1.5rem;
            background: radial-gradient(circle at top left, #1e293b 0, #020617 55%);
            border: 1px solid var(--border);
            padding: 1.5rem 1.4rem;
            position: relative;
            overflow: hidden;
        }

        .hero-card::before {
            content: "";
            position: absolute;
            inset: 0;
            background: radial-gradient(
                circle at top,
                rgba(79, 70, 229, 0.25),
                transparent 55%
            );
            opacity: 0.6;
            pointer-events: none;
        }

        .hero-card-inner {
            position: relative;
            z-index: 1;
        }

        .hero-metric-grid {
            display: grid;
            grid-template-columns: repeat(2, minmax(0, 1fr));
            gap: 0.9rem;
            margin-top: 0.9rem;
        }

        .pill {
            padding: 0.35rem 0.7rem;
            border-radius: 999px;
            border: 1px solid rgba(148, 163, 184, 0.3);
            font-size: 0.75rem;
            display: inline-flex;
            align-items: center;
            gap: 0.35rem;
            color: var(--muted);
            background: rgba(15, 23, 42, 0.8);
        }

        .pill strong {
            color: var(--text);
        }

        .pill-icon {
            width: 14px;
            height: 14px;
            border-radius: 999px;
            border: 1px solid rgba(148, 163, 184, 0.6);
            display: inline-flex;
            align-items: center;
            justify-content: center;
            font-size: 0.6rem;
        }

        .card {
            border-radius: 1.25rem;
            background: rgba(15, 23, 42, 0.9);
            border: 1px solid var(--border);
            padding: 1.25rem 1.35rem;
            margin-top: 1rem;
        }

        .card-title {
            font-size: 1rem;
            font-weight: 600;
            margin-bottom: 0.4rem;
        }

        .card-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
            gap: 1rem;
        }

        ul {
            margin: 0.35rem 0 0.5rem 1.1rem;
            padding: 0;
            color: var(--muted);
        }

        li {
            margin-bottom: 0.25rem;
        }

        .two-col {
            display: grid;
            grid-template-columns: minmax(0, 1.6fr) minmax(0, 1.5fr);
            gap: 1.25rem;
        }

        .badge {
            display: inline-block;
            font-size: 0.75rem;
            padding: 0.18rem 0.55rem;
            border-radius: 999px;
            border: 1px solid var(--accent-soft);
            background: rgba(15, 23, 42, 0.85);
            color: var(--muted);
            margin-bottom: 0.25rem;
        }

        .quote {
            border-left: 3px solid var(--accent);
            padding-left: 0.75rem;
            margin-top: 0.5rem;
            font-size: 0.95rem;
            color: var(--muted);
            font-style: italic;
        }

        .refs-list {
            font-size: 0.9rem;
        }

        .refs-list li {
            margin-bottom: 0.35rem;
        }

        footer {
            max-width: var(--max-width);
            margin: 0 auto;
            padding: 0 1.5rem 2.5rem;
            font-size: 0.8rem;
            color: var(--muted);
            border-top: 1px solid var(--border);
        }

        footer p {
            margin-top: 1rem;
        }

        @media (max-width: 900px) {
            .hero {
                grid-template-columns: minmax(0, 1fr);
            }
            .hero-card {
                order: -1;
            }
        }

        @media (max-width: 720px) {
            .nav {
                flex-direction: column;
                align-items: flex-start;
            }
            main {
                padding-inline: 1.1rem;
            }
            footer {
                padding-inline: 1.1rem;
            }
        }

        @media (prefers-reduced-motion: reduce) {
            html {
                scroll-behavior: auto;
            }
        }

        html {
            scroll-behavior: smooth;
        }
    </style>
</head>
<body>
<header>
    <div class="nav">
        <div class="nav-title">Deepfake Technology</div>
        <nav class="nav-links">
            <a href="#intro">Intro</a>
            <a href="#how-it-works">How It Works</a>
            <a href="#applications">Applications</a>
            <a href="#ethics">Ethical Lenses</a>
            <a href="#social-impact">Social Impact</a>
            <a href="#recommendations">Recommendations</a>
            <a href="#references">References</a>
        </nav>
    </div>
</header>

<main>
    <!-- HERO / OVERVIEW -->
    <section id="intro" aria-labelledby="intro-heading">
        <div class="hero">
            <div>
                <div class="hero-tag">
                    <span class="dot"></span>
                    Emerging Technology · AI-Generated Media
                </div>
                <h1 id="intro-heading">Deepfake Technology – Social Impact & Ethical Analysis</h1>
                <p class="hero-subtitle">
                    This site examines deepfake technology as a rapidly evolving form of
                    synthetic media. We focus on how it is built, how it is used in the
                    real world, and how it reshapes questions of trust, power, and
                    responsibility in digital societies.
                </p>
                <div class="meta">
                    <p>
                        <strong>Team:</strong> Mohammad Shah &amp; collaborators<br />
                        <strong>Audience:</strong> Computing &amp; information ethics faculty and researchers<br />
                        <strong>Goal:</strong> Provide a concise yet rigorous overview of deepfakes through
                        technical explanation, case studies, and ethical lenses
                        (utilitarianism and deontology).
                    </p>
                </div>
            </div>

            <aside class="hero-card" aria-label="Key points about the project">
                <div class="hero-card-inner">
                    <div class="pill">
                        <span class="pill-icon">AI</span>
                        Generative Adversarial Networks (GANs)
                    </div>
                    <p style="margin-top:0.75rem;color:var(--muted);">
                        Deepfakes are synthetic audio, image, or video artifacts generated
                        by deep learning models that can convincingly depict events
                        that never occurred.
                    </p>
                    <div class="hero-metric-grid">
                        <div class="card">
                            <div class="card-title">Research Questions</div>
                            <ul>
                                <li>What makes deepfakes technically powerful?</li>
                                <li>How do they alter political and economic systems?</li>
                                <li>Under which ethical frameworks are they justifiable?</li>
                            </ul>
                        </div>
                        <div class="card">
                            <div class="card-title">Core Argument</div>
                            <p>
                                Without strong technical, legal, and educational safeguards,
                                the social and ethical harms of deepfakes currently
                                outweigh their creative and beneficial uses.
                            </p>
                        </div>
                    </div>
                </div>
            </aside>
        </div>

        <section class="card">
            <h2>What Are Deepfakes?</h2>
            <p>
                Deepfakes are synthetic media artifacts—audio, video, and images—
                generated by artificial neural networks trained to imitate the
                statistical patterns of real people’s faces, voices, and gestures.
                With enough training data, these models can make a person appear to
                say or do something that never happened in reality.
            </p>
            <p>
                At present, the fidelity of many deepfake systems is high enough to
                deceive non-expert audiences and to circulate on social media
                platforms as if they were authentic recordings.
            </p>
        </section>
    </section>

    <!-- HOW IT WORKS -->
    <section id="how-it-works" aria-labelledby="how-heading">
        <h2 id="how-heading">How Deepfakes Work (Technical Overview)</h2>
        <div class="two-col">
            <div>
                <span class="badge">Data &amp; Representation</span>
                <p>
                    Modern deepfakes rely on large datasets of human faces and voices.
                    A model learns:
                </p>
                <ul>
                    <li>Facial geometry, expressions, and micro-movements;</li>
                    <li>Voice timbre, prosody, and accent patterns;</li>
                    <li>Temporal consistency across frames in a video sequence.</li>
                </ul>
                <p>
                    The trained model can then reconstruct or synthesize new frames
                    and audio samples that preserve identity cues while changing
                    content (what is said, where the person appears, etc.).
                </p>
            </div>
            <div>
                <span class="badge">GAN Architecture</span>
                <h3>Generative Adversarial Networks (GANs)</h3>
                <p>
                    Many deepfake systems are built using GANs, which pit two models
                    against each other:
                </p>
                <ul>
                    <li>
                        <strong>Generator:</strong> Produces synthetic images or audio
                        intended to look real.
                    </li>
                    <li>
                        <strong>Discriminator:</strong> Attempts to classify inputs as
                        “real” or “fake.”
                    </li>
                </ul>
                <p>
                    Through iterative training, the generator becomes increasingly
                    proficient at fooling the discriminator, yielding outputs that are
                    visually and acoustically convincing to human observers.
                </p>
                <p class="quote">
                    In effect, deepfakes operationalize adversarial training to
                    collapse the perceptual boundary between authentic and synthetic
                    evidence.
                </p>
            </div>
        </div>
    </section>

    <!-- APPLICATIONS -->
    <section id="applications" aria-labelledby="apps-heading">
        <h2 id="apps-heading">Real-World Applications & Case Studies</h2>

        <div class="card-grid">
            <!-- Political deepfakes -->
            <article class="card">
                <h3>Application 1: Political Deepfakes</h3>
                <p>
                    Deepfakes are increasingly deployed in electoral contexts to
                    manipulate public opinion and undermine trust in democratic
                    institutions.
                </p>
                <ul>
                    <li>
                        In 2024, an AI-generated robocall mimicking President Biden’s
                        voice urged New Hampshire voters not to participate in the
                        primary election, directly interfering with voter turnout.
                    </li>
                    <li>
                        In India’s 2024 elections, deepfake videos fabricated political
                        speeches and statements, circulating widely on social media
                        before fact-checking could catch up.
                    </li>
                </ul>
                <p>
                    These cases illustrate how cheaply generated synthetic content can
                    reach millions of citizens within hours, long before corrections or
                    debunks become visible.
                </p>
            </article>

            <!-- Social / liar's dividend -->
            <article class="card">
                <h3>Challenges in Political Use</h3>
                <p>
                    Platforms such as X and TikTok optimize for engagement and rapid
                    sharing, which amplifies emotionally charged, deceptive media.
                </p>
                <ul>
                    <li>
                        Deepfakes facilitate targeted disinformation campaigns and
                        character assassinations.
                    </li>
                    <li>
                        As high-quality forgeries proliferate, political figures may
                        dismiss authentic evidence as “fake,” benefiting from the
                        so-called <em>liar’s dividend</em>.
                    </li>
                    <li>
                        The net effect is an erosion of epistemic trust in
                        photo-, audio-, and video-based journalism.
                    </li>
                </ul>
            </article>

            <!-- Fraud deepfake -->
            <article class="card">
                <h3>Application 2: Deepfake-Enabled Fraud</h3>
                <p>
                    Deepfakes are also being weaponized for financial crime and
                    corporate social engineering.
                </p>
                <ul>
                    <li>
                        A Hong Kong–based company reportedly lost the equivalent of
                        \$25 million after fraudsters generated a deepfake video of the
                        CFO during a video conference and instructed staff to transfer
                        funds.
                    </li>
                    <li>
                        Voice cloning is widely used in impersonation scams, including
                        bank fraud and staged kidnapping calls.
                    </li>
                </ul>
                <p>
                    Traditional verification practices—such as “seeing the person” on a
                    video call—are no longer sufficient safeguards.
                </p>
            </article>

            <!-- Fraud challenges -->
            <article class="card">
                <h3>Challenges in Fraud & Security</h3>
                <ul>
                    <li>
                        Financial and identity systems are newly exposed to
                        large-scale synthetic impersonation.
                    </li>
                    <li>
                        Employees may be unable to distinguish a genuine supervisor
                        from a high-fidelity deepfake in real time.
                    </li>
                    <li>
                        Security costs rise as organizations adopt multi-factor
                        verification and continuous authentication.
                    </li>
                    <li>
                        Critically, only a few seconds of publicly available audio may
                        be sufficient to clone a person’s voice.
                    </li>
                </ul>
            </article>
        </div>
    </section>

    <!-- ETHICAL LENSES -->
    <section id="ethics" aria-labelledby="ethics-heading">
        <h2 id="ethics-heading">Ethical Evaluation</h2>
        <div class="two-col">
            <!-- Utilitarianism -->
            <article class="card">
                <h3>Ethical Lens 1: Utilitarianism</h3>
                <p>
                    Utilitarian analysis evaluates deepfake technology in terms of its
                    aggregate consequences for overall well-being.
                </p>
                <h4 style="font-size:0.98rem;margin-top:0.6rem;">Potential Benefits</h4>
                <ul>
                    <li>
                        Enhanced visual effects and cost-efficient content creation in
                        film and entertainment.
                    </li>
                    <li>
                        Accessibility applications, such as generating synthetic
                        narration or restoring speech for people who have lost their
                        voice.
                    </li>
                    <li>
                        Creative and artistic experimentation with identity and
                        representation.
                    </li>
                </ul>
                <h4 style="font-size:0.98rem;margin-top:0.6rem;">Documented Harms</h4>
                <ul>
                    <li>
                        Large-scale political disinformation and erosion of trust in
                        democratic processes.
                    </li>
                    <li>
                        Financial fraud, identity theft, and reputational damage.
                    </li>
                    <li>
                        Non-consensual explicit deepfake videos, which overwhelmingly
                        target women and constitute a form of digital sexual violence.
                    </li>
                    <li>
                        Long-term destabilization of epistemic norms: citizens can no
                        longer easily rely on “seeing is believing.”
                    </li>
                </ul>
                <p class="quote">
                    From a utilitarian perspective, the current balance of harms versus
                    benefits is negative unless strong governance and technical
                    controls are implemented.
                </p>
            </article>

            <!-- Deontology -->
            <article class="card">
                <h3>Ethical Lens 2: Deontology</h3>
                <p>
                    Deontological ethics focuses on duties, rights, and respect for
                    persons, rather than net outcomes.
                </p>
                <ul>
                    <li>
                        <strong>Violation of consent:</strong> Using someone’s face or
                        voice without explicit permission disregards their autonomy
                        over their own likeness.
                    </li>
                    <li>
                        <strong>Violation of truthfulness:</strong> Deepfakes are
                        intentionally deceptive artifacts, conflicting with duties of
                        honesty and transparency.
                    </li>
                    <li>
                        <strong>Invasion of privacy:</strong> Individuals can be placed
                        into intimate or compromising scenarios that never occurred,
                        yet feel subjectively real to viewers.
                    </li>
                    <li>
                        <strong>Manipulation of choice:</strong> Voters, consumers, and
                        employees may make decisions based on fabricated evidence.
                    </li>
                </ul>
                <p class="quote">
                    Under a deontological lens, many uses of deepfakes are morally
                    impermissible even if they produce some beneficial outcomes,
                    because they systematically violate basic rights to consent, truth,
                    and dignity.
                </p>
            </article>
        </div>
    </section>

    <!-- SOCIAL IMPACT -->
    <section id="social-impact" aria-labelledby="impact-heading">
        <h2 id="impact-heading">Broader Social Implications</h2>
        <div class="card-grid">
            <article class="card">
                <h3>Trust & Epistemic Stability</h3>
                <ul>
                    <li>
                        Widespread deepfakes contribute to a general loss of trust in
                        digital photos, videos, and audio recordings.
                    </li>
                    <li>
                        Journalistic evidence and courtroom exhibits become more
                        contestable, increasing the burden on verification.
                    </li>
                    <li>
                        Citizens may disengage from politics or adopt extreme
                        skepticism, weakening shared reality.
                    </li>
                </ul>
            </article>

            <article class="card">
                <h3>Harassment & Exploitation</h3>
                <ul>
                    <li>
                        Non-consensual explicit deepfakes disproportionately target
                        women, including minors and public figures.
                    </li>
                    <li>
                        The psychological impact includes shame, loss of control over
                        one’s image, and long-term reputational damage.
                    </li>
                    <li>
                        Existing legal frameworks for defamation and harassment are
                        often too slow or narrow to offer timely protection.
                    </li>
                </ul>
            </article>

            <article class="card">
                <h3>Polarization & Cybersecurity</h3>
                <ul>
                    <li>
                        Deepfakes can inflame political polarization by amplifying
                        scandals and conspiracy narratives.
                    </li>
                    <li>
                        Adversaries can deploy synthetic media as part of information
                        warfare and cyber-operations.
                    </li>
                    <li>
                        Organizations must redesign identity verification, incident
                        response, and crisis communication to assume synthetic media
                        is possible.
                    </li>
                </ul>
            </article>
        </div>
    </section>

    <!-- RECOMMENDATIONS -->
    <section id="recommendations" aria-labelledby="recs-heading">
        <h2 id="recs-heading">Recommendations: Technology, Policy & Society</h2>

        <div class="card-grid">
            <article class="card">
                <h3>Technical Interventions</h3>
                <ul>
                    <li>
                        <strong>Watermarking & provenance:</strong> Embed machine-readable
                        provenance signals and watermarks in AI-generated media.
                    </li>
                    <li>
                        <strong>Detection tools:</strong> Develop real-time classifiers
                        and forensic methods that can be integrated into social media
                        platforms and communication tools.
                    </li>
                    <li>
                        <strong>Model governance:</strong> Establish access controls and
                        abuse-reporting pipelines for powerful generative models.
                    </li>
                </ul>
            </article>

            <article class="card">
                <h3>Policy & Regulatory Measures</h3>
                <ul>
                    <li>
                        Enact laws requiring consent for the use of individuals’
                        likenesses and voices in synthetic media.
                    </li>
                    <li>
                        Impose meaningful penalties for harmful or deceptive deepfake
                        deployment, especially in electoral contexts.
                    </li>
                    <li>
                        Coordinate international guidelines for labeling synthetic
                        content and sharing threat intelligence.
                    </li>
                </ul>
            </article>

            <article class="card">
                <h3>Social & Educational Responses</h3>
                <ul>
                    <li>
                        Launch public awareness campaigns explaining how deepfakes are
                        made and how they circulate.
                    </li>
                    <li>
                        Incorporate media literacy into curricula so that citizens are
                        trained to question and verify digital content.
                    </li>
                    <li>
                        Provide corporate training focused on deepfake-resilient
                        verification practices and incident handling.
                    </li>
                </ul>
            </article>
        </div>

        <div class="card" style="margin-top:1.5rem;">
            <h3>Overall Conclusion</h3>
            <p>
                Deepfake technology is both powerful and deeply ambivalent. It
                enables novel creative and accessibility applications, yet it also
                destabilizes long-standing evidentiary norms and exposes individuals
                and institutions to new forms of harm.
            </p>
            <p>
                Our analysis suggests that only a combination of technical
                safeguards, legal regulation, and sustained public education can
                rebalance the technology towards ethically acceptable uses.
            </p>
        </div>
    </section>

    <!-- REFERENCES -->
    <section id="references" aria-labelledby="refs-heading">
        <h2 id="refs-heading">Selected References</h2>
        <p class="muted">
            A subset of the sources informing this project are listed below for
            quick access.
        </p>
        <ol class="refs-list">
            <li>
                Epstein, J. (2024). AI-generated robocall mimicking Biden tells New
                Hampshire Democrats not to vote. <em>NBC News</em>. Available at:
                <a href="https://www.nbcnews.com/politics/2024-election/ai-robocall-biden-new-hampshire-primary-rcna136630" target="_blank" rel="noopener noreferrer">
                    NBC News – AI Robocall
                </a>
            </li>
            <li>
                Ellis-Petersen, H. (2024). Deepfake videos flood Indian election
                campaigns. <em>The Guardian</em>. Available at:
                <a href="https://www.theguardian.com/world/2024/may/deepfake-videos-india-election" target="_blank" rel="noopener noreferrer">
                    The Guardian – India Deepfakes
                </a>
            </li>
            <li>
                BBC News. (2024). Hong Kong firm loses HK$200m in deepfake video
                call scam. Available at:
                <a href="https://www.bbc.com/news/world-asia-china-68144689" target="_blank" rel="noopener noreferrer">
                    BBC – Deepfake Fraud Case
                </a>
            </li>
            <li>
                Molla, R. (2023). AI voice cloning scams are coming for your money.
                <em>Vox</em>. Available at:
                <a href="https://www.vox.com/technology/2023/3/6/23624374/ai-voice-cloning-scams" target="_blank" rel="noopener noreferrer">
                    Vox – Voice Cloning Scams
                </a>
            </li>
            <li>
                Cole, S. (2019). Deepfake porn is a serious problem for women.
                <em>Vice</em>. Available at:
                <a href="https://www.vice.com/en/article/kzm59x/deepfake-porn-problem-women" target="_blank" rel="noopener noreferrer">
                    Vice – Deepfake Abuse
                </a>
            </li>
            <li>
                Mirsky, Y., &amp; Lee, W. (2021). The Creation and Detection of
                Deepfakes: A Survey. <em>ACM Computing Surveys</em>. Available at:
                <a href="https://dl.acm.org/doi/10.1145/3425780" target="_blank" rel="noopener noreferrer">
                    ACM – Deepfake Survey
                </a>
            </li>
            <li>
                Goodfellow, I., Pouget-Abadie, J., Mirza, M., et al. (2014).
                Generative Adversarial Nets. <em>NeurIPS</em>. Available at:
                <a href="https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" target="_blank" rel="noopener noreferrer">
                    NeurIPS – GANs
                </a>
            </li>
            <li>
                Chesney, R., &amp; Citron, D. (2019). Deepfakes and the New
                Disinformation War: The Coming Age of Post-Truth Geopolitics.
                <em>Foreign Affairs</em>.
            </li>
            <li>
                United States White House. (2023). Executive Order on Safe, Secure,
                and Trustworthy Artificial Intelligence.
            </li>
            <li>
                European Commission. (2024). EU AI Act: Rules for Artificial
                Intelligence.
            </li>
        </ol>
    </section>
</main>

<footer>
    <p>
        This site is a course project on the ethical and social implications of
        deepfake technology. All opinions expressed are for academic discussion
        and do not represent any institution.
    </p>
</footer>
</body>
</html>
